# Deep Learning â€“ CampusX Series

This repository contains learning materials and Colab notebooks that follow the **CampusX Deep Learning** playlist/tutorial series. Youâ€™ll find both conceptual notes and hands-on notebook implementations covering key deep learning topics.

---

## ðŸ“š Contents

Hereâ€™s an overview of whatâ€™s included:

| Notebook / File | Topic | Description |
|------------------|-------|-------------|
| `1.Perceptron(using_sklearn).ipynb` | Perceptron (sklearn) | Implementation of a perceptron using scikit-learn |
| `2.Perceptron_from_Scratch.ipynb` | Perceptron (scratch) | Building perceptron logic from first principles |
| `3.Perceptron_Trick(Scratch).ipynb` | Perceptron optimization trick | Improved version / variations |
| `4.Customer_Churn_Prediction.ipynb` | Churn prediction | Classification example using real / synthetic data |
| `5.Mnist_ANN.ipynb` | MNIST using ANN | Training a simple feedforward neural network on MNIST |
| `6.GRE_Admission_Project.ipynb` | Regression project | Predicting GRE admissions / scores |
| `7.BackPropogation(Regression)_from_Sratch.ipynb` | Backpropagation (regression) | Implementing backprop for regression |
| `8.BackPropogation_using_keras.ipynb` | Backpropagation (Keras) | Using Keras / TensorFlow to implement backprop |
| `9.Backpropagation_classification.ipynb` | Backpropagation (classification) | Backprop for classification tasks |
| `10_VanishingGradient.ipynb` | Vanishing gradients | Demonstration / experiments on vanishing gradient problem |
| `11_Early_stopping.ipynb` | Early stopping | Regularization by early stopping |
| `12_Feature_scaling.ipynb` | Feature scaling | Standardization, normalization, effect on training |
| `README.md` | â€” | This file |

---

## ðŸš€ Getting Started
<!--  -->
### Prerequisites

- Python 3.x  
- Pip (or conda)  
- Google Colab (or local Jupyter)  
- The following packages (you can install via `pip install ...`)

  ```bash
  pip install numpy pandas matplotlib seaborn scikit-learn tensorflow keras
